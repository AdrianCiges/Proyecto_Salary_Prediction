# Proyecto_Salary_Prediction
## CONTENIDO ğŸ“‘
[1 - Objetivo ğŸ¯](#O)<br />
[2 - PreparaciÃ³n de los datos ğŸ› ï¸](#PR)<br />
[3 - Machine Learning ğŸ¤–](#ML)<br />
 
## 1 - OBJETIVO ğŸ¯<a name="O"/>   
ğŸ’¥ Cargar, limpiar y preparar los datos para su buena adecuaciÃ³n para el proceso de machine learning. <br />
ğŸ’¥ Encontrar el modelo de machine learning que menor error (RMSE) proporcione en la predicciÃ³n de salarios de trabajos de Data. <br />
ğŸ’¥ Realizar una predicciÃ³n con el formato concreto para la competiciÃ³n de Kaggle con los compaÃ±eros del bootcamp. <br />

## 2 - PREPARACIÃ“N DE LOS DATOS ğŸ› ï¸ <a name="PR"/> 
â¤µï¸ Cargamos los archivos CSV de Data y Test (convertimos a dataframe).<br />

âŒ Eliminamos registros considerados Outliers. <br />

ğŸ§© Unimos (concatenamos) los dataframes de Data y Test.<br />

ğŸ’¼ Recategorizamos JOB_TITLE agrupando los trabajos en: Data Analyst, Data Scientist, Data Engineer.<br />

ğŸŒ Limpiamos la columna COMPANY_LOCATION quedÃ¡ndonos con los 20 paises mÃ¡s repetidos y recategorizando en "others" el resto.<br />

ğŸª™ AÃ±adimos datos nuevos al dataframe incluyendo el SALARIO_MEDIO por paÃ­s.<br />

ğŸ”¢ Limpiamos los datos y aÃ±adimos el CÃ“DIGO del paÃ­s.<br />

ğŸ¤” Calculamos nueva columna con un FACTOR que representa en quÃ© proporciÃ³n es superior el salario de Data Scientist y Data Engineer a Data Analyst ( asumimos un 15% y 20% de media en el mundo respectivamente).<br />

ğŸ’° Creamos nueva columna con SALARIO_MEDIO_PONDERADO donde multiplicamos el salario medio del paÃ­s por el factor proporcional anterior.<br />

âš™ï¸ Aplicamos Get Dummies (creamos 90 columnas).<br />

## 3 - MACHINE LEARNING ğŸ¤–<a name="ML"/> 
â–¶ï¸ Creamos una funciÃ³n para aplicar Machine Learning que nos inicializa, entrena y calcula errores de 18 modelos: <br />

&emsp; &emsp;â€¢ Linear Regression (LinReg)<br />
&emsp; &emsp;â€¢ Least Absolute Shrinkage and Selection Operator (Lasso) <br />
&emsp; &emsp;â€¢ Ridge Regression (Ridge) <br />
&emsp; &emsp;â€¢ Elastic Net Regression (Elastic)<br />
&emsp; &emsp;â€¢ Logistic Regression (LogReg)<br />
&emsp; &emsp;â€¢ Random Forest Regressor (RFR)<br />
&emsp; &emsp;â€¢ Extra Trees Regressor (ETR)<br />
&emsp; &emsp;â€¢ Support Vector Regression (SVR)<br />
&emsp; &emsp;â€¢ Click Through Rate (CTR)<br />
&emsp; &emsp;â€¢ Extreme Gradient Boosting (XGBR)<br />
&emsp; &emsp;â€¢ Gradient Boosting Regressor (GBR)<br />
&emsp; &emsp;â€¢ Light Gradient Boosted Machine (LGBMR)<br />
&emsp; &emsp;â€¢ Gaussian Naive Bayes (GNB)<br />
&emsp; &emsp;â€¢ Linear Model Bayesian Ridge (RegBR)<br />
&emsp; &emsp;â€¢ K-Nearest Neighbors (KNN)<br />
&emsp; &emsp;â€¢ Catboost Regressor (Cat)<br />
&emsp; &emsp;â€¢ Stochastic Gradient Descent Regressor (SGD)<br />
&emsp; &emsp;â€¢ Kernel Ridge Regression (KerRid)<br />

â€¼ï¸ Detectamos que el modelo que minimiza el error es KernelRidge.<br />

â¤´ï¸ Calculamos predicciones con KernelRidge, creamos archivo de sample y subimos los datos.<br />

